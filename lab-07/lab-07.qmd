---
title: "Lab 7 - Web Scraping and Regular Expressions"
format: html
editor: visual
embed-resources: true
toc: true
---

### **Learning goals**

-   Use real-time data pulled from the internet.

-   Use regular expressions to parse the information.

-   Practice your GitHub skills.

### **Lab description**

In this lab (due Tuesday Mar 3rd at 8:30am EST), we will be working with the [NCBI API](https://www.ncbi.nlm.nih.gov/home/develop/api/) to make queries and extract information using XML and regular expressions. For this lab, we will be using the `httr`, `xml2`, and `stringr` R packages.

```{r}
library(httr)
library(stringr)
```

## **Question 1: How many Sars-Cov-2 papers?**

We will build an automatic counter of Sars-Cov-2 papers available on PubMed. You will need to apply XPath as we did during the lecture to extract the number of results returned by PubMed when you search for “sars-cov-2.”

The following URL will perform the search: <https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2>

And you can find the total number of results in the top left corner of the search results.

```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")

# Finding the counts
counts <- xml2::xml_find_first(website, "//meta[@name = 'log_resultcount']")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
totalcount <- stringr::str_extract(counts, "[0-9]+")

# Removing any commas/dots so that we can convert to numeric
totalcount <- gsub('[./]+', '', totalcount)

totalcount <- as.numeric(totalcount)
print(totalcount)
```

## **Question 2: Get article abstracts and authors**

That’s quite a few articles, so let’s narrow our focus by including “michigan” in our search:

<https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2%20michigan>

In your web browser, use the slider on the left to narrow your search down to just the years 2020 and 2021.

Now we will download the abstracts and author information for all of these articles. Under the search bar, click “Save,” set the Selection to “All results,” set the Format to “Abstract (text),” and click “Create file.” This should start downloading a large-ish text file (`abstract-sars-cov-2-set.txt`)

We can read this into R with the following code. We need to do some formatting because the abstracts are separated by two blank lines. COMPLETE THE FOLLOWING LINES OF CODE.

```{r}
# read in text, each line is a separate character
abstracts <- readLines('~/Downloads/umich/BIOSTAT620/BIOSTAT620labs/data/abstract-sars-cov-2-set.txt', warn = FALSE)
# combine all text into one character
abstracts <- paste(abstracts, collapse = "\n")
# split the text whenever 3 new lines occur in a row (indicating two blank lines)
abstracts <- strsplit(abstracts, "\n\n\n")[[1]]

# replace any remaining "\n" symbols with spaces
abstracts <- gsub("\\n", " ", abstracts)
# replace multiple spaces with single space
abstracts <- gsub("\\s+", " ", abstracts)
```

## **Question 3: Distribution of universities**

Look through the first couple abstracts and see how the author affiliations are formatted.

Using the function `stringr::str_extract_all()` applied on `abstracts`, capture all the terms of the form:

1.  “… University”

2.  “… State University”

3.  “University of …”

4.  “… Institute of …”

Write a regular expression that captures all such instances.

```{r}
institution <- str_extract_all(
  abstracts,
  "\\b([A-Z][A-Za-z&\\-]+\\sUniversity|[A-Z][A-Za-z&\\-]+\\sState\\sUniversity|University\\sof\\s[A-Z][A-Za-z&\\-]+|[A-Z][A-Za-z&\\-]+\\sInstitute\\sof\\s[A-Z][A-Za-z&\\-]+)\\b"
  )
institution <- unlist(institution)
top_institution <- table(institution)

top_10 <- top_institution[order(top_institution, decreasing = TRUE)][1:10]
top_10
```

This can be improved by capturing more phrases after "University of" and before "University". We can see this affecting our results with "Hopkins University" being captured instead of "Johns Hopkins University" and "The University" being captured.

## **Question 4: Make a tidy dataset**

We want to build a dataset which includes the journal, article title, authors, and affiliations for each paper. In order to do this, we will go back to the original input, but not do all the formatting we did previously.

```{r}
# read in text, each line is a separate character
abstracts <- readLines('~/Downloads/umich/BIOSTAT620/BIOSTAT620labs/data/abstract-sars-cov-2-set.txt', warn = FALSE)
# combine all text into one character
abstracts <- paste(abstracts, collapse = "\n")
# split the text whenever 3 new lines occur in a row (indicating two blank lines)
abstracts <- strsplit(abstracts, "\n\n\n")[[1]]
```

Now extract the journal title for each article. This is in the first line of each entry, immediately after the citation number (ie. “1.”) and before the year (e.g. “2020”). Notice that we are using `str_extract` rather than `str_extract_all`, because we only want (at most) one result per entry.

```{r}
journal <- str_extract(abstracts, "^\\d+\\.\\s(.*?)\\s\\d{4}")
# remove citation number
journal <- gsub("^\\d+.", "", journal)
head(journal, 5)
```

Now we’re going to extract the title of each article. This is the second non-empty line in each entry, with blank lines before and after. We could do this with a regular expression, but instead, we’ll use the `strsplit()` function, as we did above.

```{r}
titles <- sapply(abstracts, function(x){
  unlist(strsplit(x, split = "\n\n"))[2]
}, USE.NAMES = FALSE)
```

Note that each title could still contain a *single* `\n` symbol, in the case of a particularly long title. We could remove those as we did the first time we read in the data.

```{r}
# replace any remaining "\n" symbols with spaces
titles <- gsub("\\n", " ", titles)
# replace multiple spaces with single space
titles <- gsub("\\s+", " ", titles)
head(titles, 10)
```

Use the same technique to extract the list of authors and call this object `authors`.

```{r}
authors <- sapply(abstracts, function(x){
  unlist(strsplit(x, split = "\n\n"))[3]
}, USE.NAMES = FALSE)

# replace any remaining "\n" symbols with spaces
authors <- gsub("\\n", " ", authors)
# replace multiple spaces with single space
authors <- gsub("\\s+", " ", authors)
head(authors, 5)
```

Now use a regular expression to extract the author affiliations section for each abstract. This always starts with a blank line followed by “Author information:” and ends with another blank line.

```{r}
affiliations <- str_extract(abstracts, "(?s)\\nAuthor\\sinformation:(.*?)\\n\\s*\\n")
head(affiliations, 1)

# remove label
affiliations <- str_replace(affiliations, "^\\nAuthor information:\\s*", "")
# replace any remaining "\n" symbols with spaces
affiliations <- gsub("\\n", " ", affiliations)
# replace multiple spaces with single space
affiliations <- gsub("\\s+", " ", affiliations)

head(affiliations, 3)
```

Finally, put everything together into a single `data.frame` and use `knitr::kable` to print the first five results. Hint: cut off the affiliations and authors to each be at most 100 characters per abstract for better rendering.

```{r}
authors_cut <- str_trunc(authors, width = 100)
affiliations_cut <- str_trunc(affiliations, width = 100)

papers <- data.frame(
  Journal = journal,
  Title = titles,
  Authors = authors_cut,
  Affiliations = affiliations_cut
)
knitr::kable(papers[1:5, ])
```
